
%
%  $Description: Author guidelines and sample document in LaTeX 2.09$ 
%
%  $Author: ienne $
%  $Date: 1995/09/15 15:20:59 $
%  $Revision: 1.4 $
%

\documentclass[times, 10pt,twocolumn]{article} 
\usepackage{latex8}
\usepackage{times}
\usepackage{graphicx}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{amsmath}

%\documentstyle[times,art10,twocolumn,latex8]{article}
%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
\pagestyle{empty}

%------------------------------------------------------------------------- 
\begin{document}

\title{Solving Sokoban Using Q-Learning}

\author{Joseph Herkness\\
Oakland University\\
jpherkness@oakland.edu\\
\and
Joshua Herkness\\
Oakland University\\
jrherkness@oakland.edu\\
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
   This paper describes a Q-Learning based algorithm used to determine the solution to a sokoban puzzle. The nature of Sokoban, as a finite decision problem, means that a Reinforcement Learning approach can be used to identify the problems optimal policy. Q-Learning is a Reinforcement Learning technique that constructs the optimal policy through recording the utility of executing an action on a given state. 
\end{abstract}

% Keywords
\providecommand{\keywords}[1]{\textbf{\textit{Keywords---}} #1}
\keywords{Sokoban, Solver, Q-Learning, Reinforcement Learning, Machine Learning, Reward, Algorithm}

%------------------------------------------------------------------------- 
\Section{Introduction}

Q-Learning is a form of model-free reinforcement learning that provides agents with the capability to develop a strategy for solving an instance of a problem. Model-free learning differs from model-based learning in that it does not require a pre constructed model of the problem in order to derive a solution. Instead, Q-Learning derives the solution through exploring the problem space, much like many dynamic programming concepts. In this paper, a Q-Learning algorithm, that identifies the solution to a Sokoban puzzle, will be explained. A Sokoban puzzle is a grid based logic puzzle where a player pushes boxes around a warehouse, the ultimate goal being to position these boxes on top of every goal. The typical Q-Learning approach will be improved upon through an analysis of potential traps in each state, as well as adjustments to the learning rate and discount factor through dynamic modification as episodes progress. 

%------------------------------------------------------------------------- 
\SubSection{Motivation}

Sokoban has been proven to be a PSPACE-Complete problem [1]. Other approaches to this problem include BFS, DFS, and solvers employing heuristic functions such as A* search.

The problem of solving a sokoban puzzle is often compared to the real world problem of programming an autonomous robot to work in a warehouse. Such a robot would be required to navigate the warehouse, as well as perform its designated tasks, using only the information in its immediate vicinity. If such a robot’s task were to move crates into a storage location, then the two problems would be synonymous. In this case, a similar solution could be used to solve both problems. If Q-Learning proves to be a valid solution to the sokoban problem, it’s application to real world problems could prove to be limitless.

If, however, there was more than a single agent acting within the environment, the basic Q-Learning approach will fail. For example, imagine a situation where multiple autonomous robots are each moving boxes within a warehouse from location to location. If a basic Q-Learning approach was implemented, these robots would begin to interfere with one another. To fix this, the robots would need some way to communicate learned information to and from one another [7].

%------------------------------------------------------------------------ 
\Section{Sokoban}

Sokoban is two dimensional puzzle game. Each Sokoban level consists of a rectangular grid representing a warehouse. The warehouse contains many different entities, such as boxes, goals, walls, and the player themselves. Each entity is restricted to the two dimensional grid of the warehouse. In order to solve the puzzle, the player must push each box such that it covers one of the goals. When every goal has been covered by a box, the puzzle is considered solved. A valid Sokoban puzzle must contain a number of boxes equal to the number of goals. Each action that the player takes has the potential of placing the level in a deadlock. If a deadlock is created, the level is no longer solvable, and the player has lost.

The level shown in figure \ref{fig:b} is the simplest solvable sokoban level that can be made. The state on the right is the initial state, and the state on the left is the final state. The solution to the puzzle is to simply move to the right one space. This action pushes the box onto the goal, solving the level.

\begin{figure}[h] 
  \centering
     \includegraphics[width=0.7\linewidth]{images/basic_unsolved_solved.png}
  \caption{The simplest solvable sokoban level (left) and the solved state (right).}
  \label{fig:b}
\end{figure}

%------------------------------------------------------------------------- 
\SubSection{Rules}

During each turn, the player has the ability to move in one of four directions - up, down, left, or right. A square is considered empty if it does not contain a wall or box. If the square corresponding to the direction moved contains a box, and the next square is considered empty, the box is pushed into that square. The player is allowed to move into any empty square. When every goal has been covered by a box, the puzzle is considered solved, and the game ends.

%------------------------------------------------------------------------- 
\SubSection{Deadlocks}

A deadlock is a configuration of boxes that results in an unsolvable level [1]. A state is considered deadlocked if it contains at least one deadlock. Any action that the player takes has the potential of placing the level in a deadlocked state. If such a state is encountered, any attempt to solve the level should no longer be pursued, even if there are still valid actions that can be applied to the deadlocked state. While there are numerous types of deadlocks, two that are necessary for complete deadlock detection are simple deadlocks and freeze deadlocks. Implementation of the detection of these two deadlock types results in the ability to detect whether or not a action creates a deadlocked state. 

%------------------------------------------------------------------------- 
\SubSection{Simple Deadlocks}

Simple deadlocks are squares in the level that create a deadlock whenever a box is pushed into them. Pushing a block into a simple deadlock square creates a deadlock because the box in that square can no longer be pushed to a goal. These simple deadlock squares are independent of the positions of the boxes in the level, and do not change. This means that simple deadlock squares can be identified when the initial level is loaded. 

The algorithm for identifying simple deadlock squares is rather intuitive. By definition, if a box cannot be pushed from a square to one of the goals, the square is considered to be a simple deadlock square. Likewise, if a box cannot be pulled from one of the goals to that square, it means the same thing. This mean that all valid squares can be identified by removing all boxes from the level, placing a box on each goal square, pulling the box away in all directions, and marking all reachable squares as visited. Every square that is not marked as visited is a simple deadlock square. 

If we consider the level in figure \ref{fig:sd}, where the simple deadlock squares are crossed off, the algorithm becomes a little more clear. First we remove all boxes from the level. We then place an imaginary box on the goal and pull it in each direction. Since we cannot pull the box up, the space directly above the goal is a simple deadlock space. Since we cannot pull the box down, the square directly below the goal is a simple deadlock square. However, since we can pull the box to the left, the square directly to the left of the goal is considered a valid square. This square is then marked as visited, and we repeat the algorithm on this new square. 

\begin{figure}[h] 
  \centering
     \includegraphics[width=0.5\linewidth]{images/simple_deadlock.png}
  \caption{The simple deadlock squares are identified in this level and marked.}
  \label{fig:sd}
\end{figure}

The result of performing such an algorithm is the identification of all simple deadlock squares. When a box is moved, we can check the see if the square it is moved into is a simple deadlock square. If it is a simple deadlock square, we know there is a deadlock and the level can no longer be solved.

\begin{algorithm}
  \caption{Identifying simple deadlocks}
\begin{algorithmic}[1]
  \Function {IdentifySimpleDeadlocks}{}
    \State $stack\gets \textnormal{goals}$
    \State $visited\gets \textnormal{empty set}$
    \While { stack not empty }
      \State $position\gets stack.pop()$
      \State visited.add(position)
      \For {direction = up, down, left, right}
        \If {can pull position in direction}
          \State $valid\gets \textnormal{move position in direction}$
          \State stack.add(position)
        \EndIf
      \EndFor
    \EndWhile
    \State \Return all squares - visited
  \EndFunction
  \end{algorithmic}
\end{algorithm}

%------------------------------------------------------------------------- 
\SubSection{Freeze Deadlocks}

Freeze deadlocks are configurations of boxes and walls that result in a deadlocked state. Unlike the simple deadlock squares, freeze deadlocks depend on the position of boxes in the level. This means that we must check for a freeze deadlock anytime a box is moved in the level. 

The level in figure \ref{fig:f} demonstrates an example of a freeze state. Notice how if either box is pushed up, it will be moved into a simple deadlock square. Since we can not push either box up, they are both blocked along the vertical axis. Since the blocks are side by side, any attempt to push the boxes left or right will fail. This mean each box is blocked along the vertical axis. Since both boxes are blocked along the vertical and horizontal axis, the entire configuration is considered a freeze deadlock, meaning no box in the configuration can be moved. However, if all boxes in the configuration are on goals, the level is considered to be in a semi-solved state, and there is no freeze deadlock. An example of such a configuration can be seen in figure \ref{fig:fok}.

\begin{figure}[h] 
  \centering
     \includegraphics[width=0.5\linewidth]{images/freeze_deadlock.png}
  \caption{The configuration of boxes in this level creates a freeze deadlock.}
  \label{fig:f}
\end{figure}

\begin{figure}[h] 
  \centering
     \includegraphics[width=0.5\linewidth]{images/freeze_deadlock_ok.png}
  \caption{Since the boxes in this level are frozen on top of the goals, no freeze deadlock is created.}
  \label{fig:fok}
\end{figure}

The algorithm for detecting freeze deadlocks is rather intuitive, since we only need to check whether a box can be pushed. Since a freeze deadlock is created after pushing a block, it is only neccessary to check if the pushed box is frozen, and possibly the boxes around the pushed box. The following algorithm will identify whether or not pushing a box into a position will create a freeze deadlock.

\begin{algorithm}
  \caption{Identifying freeze deadlocks}
\begin{algorithmic}[1]
  \State $frozen \gets \textnormal{empty set}$
  \State $visited \gets \textnormal{empty set}$
  \Function {Frozen}{position}
    \State $\textnormal{add position to visited}$
    \If { wall on left or right }
      \State $h\gets true$
    \ElsIf { simple deadlock on on left and right }
      \State $h\gets true$
    \ElsIf { box on left or right }
      \State $h\gets \textnormal{Frozen(left) or Frozen(right)}$
    \EndIf

    \If { wall on up or down }
      \State $v\gets true$
    \ElsIf { simple deadlock on on up and down }
      \State $v\gets true$
    \ElsIf { box on up or down }
      \State $v\gets \textnormal{Frozen(up) or Frozen(down)}$
    \EndIf
    \State \Return $\textnormal{h and v and all frozen on goals}$
  \EndFunction
  \end{algorithmic}
\end{algorithm}

%------------------------------------------------------------------------- 
\SubSection{Notation}

%------------------------------------------------------------------------- 
\Section{Related Work}

%------------------------------------------------------------------------- 
\Section{Q-Learning}

Q-Learning is one of the more prominent reinforcement learning techniques. This techniques can be used to find the optimal actions that should be taken for any given markov decision process. A markov dicision process is a mathematical framework used to model decision making problems where the decision are under the agent's control. At any given state $s$ the agent can perform an action $a$. The result of taking action $a$ is the new state $s\prime$. The agent is then awarded a reward of $R_a(s, s\prime)$ for taking action $a$. The agent’s goal is to develop a strategy for solving the problem that results in the highest reward.

Q-Learning allows a single agent, aware of its current state, to develop a strategy through exploration. This is accomplished through consideration of past states and future reward for performing all possible actions on the current state. An agent is placed into its environment at an initial state $s$ and begins by taking some arbitrary action $a$. Once this action is taken, the Q-value for the given state (s), after taking that action (a), is updated. The agent is said to have learned from this action (a), and will use this learned reward in the future if it encounters this state (s) again. The Q-value for taking an action (a) at a state (s) is defined as \cite{ex1}:

\begin{equation}
\begin{split}
Q(s_t, a_t) \gets &Q(s_t, a_t) \\
                  &+ \alpha \cdot [R_a(s_t, s_{t+1}) \\
                  &+ \gamma \cdot max_aQ(s_{t+1}, a) - Q(s_t, a_t)]
\end{split}
\end{equation}

Where α is the Learning Rate of the agent, and γ is the Discount Factor of the agent.

%------------------------------------------------------------------------- 
\SubSection{Rewards}

Rewards are one of the major building blocks of Q-Learning.  If you recall, a reward is given to an agent once an action $a$ is perfomed on a state $s_t$.  Many Q-Learning algorithms implement a reward function which, depending on the outcome state $s_{t+1}$, will result in a numberical reward.  These rewards will fall into either one of two categories, negative or positive.  A positive reward should be given when a desirable outcome is acheived after action $a_t$ is performed at state $s_t$, resulting in an outcome state $s_{t+1}$.  The nature of the rewards implementation in the Q-Learning algorithm will cause the agent to perfer taking action $a$ at state $s_t$ again.  A negative reward should be given when a undesirable outcome is acheived after action $a$ is performed at state $s_t$, resulting in an outcome state $s_{t+1}$.  Converse to the effect that the positive reward will have, the negative reward will cause the agent to avoid taking action $a$ at state $s_t$ again.  The formula for this reward is generally written as a difference function between the intial state $s_t$ and outcome state $s_{t+1}$ as seen in equation \ref{eq:reward}.

\begin{equation}
reward \gets R_a(s_t, s_{t+1})
\label{eq:reward}
\end{equation}

A full list of the finalized rewards in our soluction can be found in table \ref{table:rewards}.  We found that positive rewards should be given very rarely, and only in extremely desirable circumstances.  For instance, we gave the highest (positive) reward when a box was pushed onto a goal.  In the inital design of our solution, we gave the agent a reward of 0 for moving, because we saw no immediate negative or positive effects from this.  However, in our final implementation, we gave a negative reward to an agent for moving because this encouraged exploration and discouraged 'wondering'.  In the development of a successful Q-Learning algorithm for the Sokoban puzzle, we found that the adjustment of our rewards had a significant impact on the success rate of the solver.

\begin{table}[htbp]
\begin{tabular}{l l} \hline\hline
Reward Value & Reward Type \\ \hline
+1.00 & Box pushed onto goal \\
-0.01 & Box pushed \\
-0.15 & Agent moved \\
-0.15 & Box pushed against wall \\
-0.15 & Box pushed against box \\
-0.15 & Box pushed from one goal to another \\
-0.20 & Box pushed off goal \\ 
-1.00 & Agent moved into wall \\
-1.00 & Agent moved into immovable box \\
-1.00 & Agent created simple deadlock \\
-1.00 & Agent created freeze deadlock \\ \hline\hline
\end{tabular}
\caption{Rewards awarded to the Sokoban agent during Q-Learning}
\label{table:rewards}
\end{table}

%------------------------------------------------------------------------- 
\SubSection{Algorithm}

The following Q-Learning algorithm [3][6] will be used to determine the optimal strategy for solving a given Sokoban puzzle. In the algorithm, an episode represents one iteration from initial state to terminal state. Numerous episodes will be executed in sequence, allowing the agent to learn which actions produce the greatest reward. The greater the number of episodes, the more optimal the solution will become. When the q-values converge, meaning they stop changing between episodes, the optimal solution will be found.

One of the most critical aspects of the algorithm is identifying if the current state is a terminal state. In the case of Sokoban, a terminal state is defined as a state where the puzzle is solved, or a state that is no longer solvable. The following predicates will be used to [1] analyze the state and determine if it has reached a terminal state.

\begin{algorithm}
  \caption{Solver for Sokoban using q learning}
  \begin{algorithmic}[2]
    \Function {QLearning}{state, episodes}
      \State $Q\gets \textnormal{empty set}$
      \For {episode = episodes}
        \State $Q\gets \textnormal{RunEpisode(Q)}$
      \EndFor
      \Return Q
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Runs a single episode of q learning}
  \begin{algorithmic}[3]
    \Function {RunEpisode}{state, Q}
      \State $state\gets \textnormal{initial state}$
      \While {not Terminal?(state)}
        \State $action\gets \textnormal{MaximizeAction(state, Q)}$
        \State $resultState\gets \textnormal{TakeAction(state, action)}$
        \State $\textnormal{update Q value}$
      \EndWhile
      \State \Return Q
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Returns the maximum q value reachable from a state in a single action}
  \begin{algorithmic}[4]
    \Function {MaximizeQ}{state, Q}
      \State $max\gets -\infty$
      \For {action = up, right, down, left}
        \If {$Q(state, action) > max$}
          \State $max\gets Q(state, action)$
        \EndIf
      \EndFor
      \State \Return $max$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Returns the action which will acheive the maximum q value reachable from a state}
  \begin{algorithmic}[5]
    \Function {MaximizeAction}{state, Q}
      \State $max\gets -\infty$
      \State $actions\gets \textnormal{empty set}$
      \For {action = up, right, down, left}
        \If {$Q(state, action) > max$}
          \State $max\gets \textnormal{Q(state, action)}$
          \State $actions\gets \textnormal{set with action}$
        \Else
          \State $actions\gets actions + action$
        \EndIf
      \EndFor
      \State \Return random action from $actions$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Returns the resulting state after an action is taken on an intial state}
  \begin{algorithmic}[6]
    \Function {MaximizeQ}{state, action}
      \State \Return state after action is taken
    \EndFunction
  \end{algorithmic}
\end{algorithm}

%------------------------------------------------------------------------- 
\Section{Experimental Analysis}

In order to evaluate the performance of our solver, we selected three levels with various difficulty. The levels were classified into three groups, easy, medium, and hard. These difficulties are based on the minimum number of moves required to solve the puzzle, and the number of boxes in the puzzle. In order to understand the exact capabilities of the solver, the following parameters were measured after each episode: the number of moves the agent takes, the average reward the agent received, whether or not the agent was able to solve the puzzle, and the time it took for the agent to complete the episode. 

\begin{figure}[h] 
  \centering
     \includegraphics[width=\linewidth]{images/easy_graph.png}
  \caption{Results for solving an easy level.}
  \label{fig:e}
\end{figure}

The results from figure \ref{fig:e} demonstrate the solver's ability to identify the optimal action policy after only 100 episodes. Although the number of moves the solver used to solve the puzzle stabilized at 11 after only 70 episodes, the average reward for each episode did not stabilize until episode 100. Instead, the average reward continued to increase until its maximum at episode 100. This demonstrates the ability of the solver to refine the action policy through continuous exploration. Although the agent identified an action policy that would solve the puzzle, they continued to search for an even better action policy. However, since no better action policy was found, the agent continued to use the action policy that they knew would produce the greatest reward.

\begin{figure}[h] 
  \centering
     \includegraphics[width=\linewidth]{images/medium_graph.png}
  \caption{Results for solving an medium level.}
  \label{fig:m}
\end{figure}

The level that produced the results in figure \ref{fig:m} was a bit more challenging than the level that produced the results to figure \ref{fig:e}. Though the number of moves required to solve the level was only 9, compared to the 11 moves required to solve the other level, the number of boxes increased to two. This increases the complexity of the level greatly. After about 140 episode, the solver was able to identify the optimal action policy.  

\begin{figure}[h] 
  \centering
     \includegraphics[width=\linewidth]{images/hard_graph.png}
  \caption{Results for solving an hard level.}
  \label{fig:h}
\end{figure}

The level that produced the results in figure \ref{fig:h} takes a minimum of 30 moves to solve and contained two boxes, making it the most complicated level that was tested. The solver was able to identify the optimal action policy after about 400 episodes. The results from this data provided the most insight into the solvers performance. The number of moves that the solver used began small, increasing until about episode 80, then decreasing until about episode 400. If the solvers goal is to minimize the number of moves, why was the number of moves increasing? Though the number of moves was increasing, the agent was not solving the level. instead, the agent was creating deadlocks and the episode was ending early. This allowed the agent to identify the actions that created deadlocks and increase the number of moves they were able to take. When all deadlocks were identified, after approximately 80 episodes, the agent could then refine the action policy and reduce the overall moves they needed to take.

\SubSection{Rewards}

\SubSection{Learning Rate and Discount Factor}

%------------------------------------------------------------------------ 
\Section{Conclusions}

%------------------------------------------------------------------------- 
\bibliographystyle{latex8}
\bibliography{latex8}

\begin{thebibliography}{9}
\bibitem{culberson} 
J. Culberson. 
\textit{Sokoban is PSPACE-complete}.
1997.

\bibitem{jarusek}
P. Jarušek and R. Pelánek.
Difficulty rating of sokoban puzzle
in Proc. of the Fifth Starting AI Researchers' Symposium (STAIRS 2010).
Dec, 2010. pp.~140-150

\bibitem{russell}
S. J. Russell and P. Norvig.
\textit{Artificial intelligence: a modern approach}.
Upper Saddle River: Prentice-Hall, 2010.

\bibitem{littman} 
M. L. Littman.
Markov games as a framework for multi-agent
reinforcement learning in Proceedings of the eleventh international
conference on machine learning. Vol 157. pp 157-163. 1994

\bibitem{guo} 
M. Guo, Y. Liu and J. Malec
\textit{A new Q-learning algorithm based on the metropolis criterion}.
IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol.~34, no. 5, pp.~2140-2143, Oct. 2004.

\bibitem{ming} 
T. Ming.
\textit{Multi-agent reinforcement learning: Independent vs.~cooperative agents.}.
Proceedings of the tenth international conference on machine learning. 1993.

\end{thebibliography}

\end{document}


