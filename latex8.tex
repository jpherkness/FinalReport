
%
%  $Description: Author guidelines and sample document in LaTeX 2.09$ 
%
%  $Author: ienne $
%  $Date: 1995/09/15 15:20:59 $
%  $Revision: 1.4 $
%

\documentclass[times, 10pt,twocolumn]{article} 
\usepackage{latex8}
\usepackage{times}
\usepackage[font=normalsize, labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{amsmath}
\usepackage{url}

%\documentstyle[times,art10,twocolumn,latex8]{article}
%------------------------------------------------------------------------- 
% take the % away on next line to produce the final camera-ready version 
\pagestyle{empty}

%------------------------------------------------------------------------- 
\begin{document}

\title{Solving Sokoban}

\author{Joseph Herkness\\
Oakland University\\
jpherkness@oakland.edu\\
\and
Joshua Herkness\\
Oakland University\\
jrherkness@oakland.edu\\
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
   This paper describes a Q-Learning based algorithm used to determine the solution to a sokoban puzzle. The nature of Sokoban, as a finite decision problem, means that a Reinforcement Learning approach can be used to identify the problems optimal policy. Q-Learning is a Reinforcement Learning technique that constructs the optimal policy through recording the utility of executing an action on a given state. 
\end{abstract}

% Keywords
\providecommand{\keywords}[1]{\textbf{\textit{Keywords---}} #1}
\keywords{Sokoban, Solver, Q-Learning, Reinforcement Learning, Machine Learning, Reward, Algorithm}

%------------------------------------------------------------------------- 
\Section{Introduction}

Q-Learning is a form of model-free reinforcement learning that provides agents with the capability to develop a strategy for solving an instance of a problem. Model-free learning differs from model-based learning in that it does not require a pre constructed model of the problem in order to derive a solution. Instead, Q-Learning derives the solution through exploring the problem space, much like many dynamic programming concepts. In this paper, a Q-Learning algorithm, that identifies the solution to a Sokoban puzzle, will be explained. A Sokoban puzzle is a grid based logic puzzle where a player pushes boxes around a warehouse, the ultimate goal being to position these boxes on top of every goal. The typical Q-Learning approach will be improved upon through an analysis of potential traps in each state, as well as adjustments to the learning rate and discount factor through dynamic modification as episodes progress. 

%------------------------------------------------------------------------ 
\Section{Sokoban}

Sokoban is two dimensional puzzle game. Each Sokoban level consists of a rectangular grid representing a warehouse. The warehouse consists of squares which can contain four types of objects: walls, boxes, goals, and the player. If a square does not contain any of these objects, it is considered empty. Each object is restricted to the two dimensional grid of the warehouse. In order to solve the puzzle, the player must push each box such that it covers one of the goals. When every goal has been covered by a box, the puzzle is considered solved. A valid Sokoban level, such as the one depicted in figure \ref{fig:b}, must contain a number of boxes equal to the number of goals. Each action that the player takes has the potential of placing the level in a deadlock. If a deadlock is created, the level is no longer solvable, and the player has lost.

\begin{figure}[h] 
  \centering
     \includegraphics[width=0.7\linewidth]{images/basic_unsolved_solved.png}
  \caption{The simplest solvable sokoban level (left) and the solution (right).}
  \label{fig:b}
\end{figure}

The level depicted in figure \ref{fig:b} is the simplest solvable sokoban level that can be created. The solution to the puzzle is to simply move the player to the right one square. This action pushes the box onto the goal. Since there is only one box, and the box is covering a goal, the level is considered solved.

%------------------------------------------------------------------------- 
\SubSection{Rules}

The rules of the game are rather simple. During each turn, the player has the ability to move in one of four directions: up, down, left, or right. A square is considered empty if it does not contain a wall or box. If the square corresponding to the direction moved contains a box, and the next square is considered empty, the box is pushed into that square. The player is allowed to move into any empty square. When every goal has been covered by a box, the puzzle is considered solved, and the game ends.

%------------------------------------------------------------------------- 
\SubSection{Deadlocks}

Another important aspect of Sokoban is the existance of deadlocks. A deadlock is a configuration of objects that results in an unsolvable level [1]. A state is considered deadlocked if it contains at least one deadlock. Any action that the player takes has the potential of placing the level in a deadlocked state. If such a state is encountered, any attempt to solve the level should no longer be pursued, even if there are still valid actions that can be applied to the deadlocked state. While there are numerous types of deadlocks, two that are necessary for complete deadlock detection are simple deadlocks and freeze deadlocks. Implementation of the detection of these two deadlock types results in the ability to detect whether or not a action creates a deadlocked state. 

%------------------------------------------------------------------------- 
\SubSection{Simple Deadlocks}

Simple deadlocks are squares in the level that create a deadlock whenever a box is pushed into them. Pushing a block into a simple deadlock square creates a deadlock because the box in that square can no longer be pushed to a goal. These simple deadlock squares are independent of the positions of the boxes in the level, and do not change. This means that simple deadlock squares can be identified when the initial level is loaded. 

The algorithm for identifying simple deadlock squares is rather intuitive. By definition, if a box cannot be pushed from a square to one of the goals, the square is considered to be a simple deadlock square. Likewise, if a box cannot be pulled from one of the goals to that square, it means the same thing. This mean that all valid squares can be identified by removing all boxes from the level, placing a box on each goal square, pulling the box away in all directions, and marking all reachable squares as visited. Every square that is not marked as visited is a simple deadlock square. \cite{Wiki}

If we consider the level in figure \ref{fig:sd}, where the simple deadlock squares are crossed off, the algorithm becomes a little more clear. First we remove all boxes from the level. We then place an imaginary box on the goal and pull it in each direction. Since we cannot pull the box up, the space directly above the goal is a simple deadlock space. Since we cannot pull the box down, the square directly below the goal is a simple deadlock square. However, since we can pull the box to the left, the square directly to the left of the goal is considered a valid square. This square is then marked as visited, and we repeat the algorithm on this new square. 

\begin{figure}[h] 
  \centering
     \includegraphics[width=0.5\linewidth]{images/simple_deadlock.png}
  \caption{The simple deadlock squares are identified in this level and marked.}
  \label{fig:sd}
\end{figure}

The result of performing such an algorithm is the identification of all simple deadlock squares. When a box is moved, we can check the see if the square it is moved into is a simple deadlock square. If it is a simple deadlock square, we know there is a deadlock and the level can no longer be solved.

\begin{algorithm}
  \caption{Identifying simple deadlocks}
\begin{algorithmic}[1]
  \Function {IdentifySimpleDeadlocks}{}
    \State $stack\gets \textnormal{goals}$
    \State $visited\gets \textnormal{empty set}$
    \While { stack not empty }
      \State $position\gets stack.pop()$
      \State visited.add(position)
      \For {direction = up, down, left, right}
        \If {can pull position in direction}
          \State $valid\gets \textnormal{move position in direction}$
          \State stack.add(position)
        \EndIf
      \EndFor
    \EndWhile
    \State \Return all squares - visited
  \EndFunction
  \end{algorithmic}
\end{algorithm}

%------------------------------------------------------------------------- 
\SubSection{Freeze Deadlocks}

Freeze deadlocks are configurations of boxes and walls that result in a deadlocked state. Unlike the simple deadlock squares, freeze deadlocks depend on the position of boxes in the level. This means that we must check for a freeze deadlock anytime a box is moved in the level. 

The level in figure \ref{fig:f} demonstrates an example of a freeze state. Notice how if either box is pushed up, it will be moved into a simple deadlock square. Since we can not push either box up, they are both blocked along the vertical axis. Since the blocks are side by side, any attempt to push the boxes left or right will fail. This mean each box is blocked along the vertical axis. Since both boxes are blocked along the vertical and horizontal axis, the entire configuration is considered a freeze deadlock, meaning no box in the configuration can be moved. However, if all boxes in the configuration are on goals, the level is considered to be in a semi-solved state, and there is no freeze deadlock. An example of such a configuration can be seen in figure \ref{fig:fok}.

\begin{figure}[h] 
  \centering
     \includegraphics[width=0.5\linewidth]{images/freeze_deadlock.png}
  \caption{The configuration of boxes in this level creates a freeze deadlock.}
  \label{fig:f}
\end{figure}

\begin{figure}[h] 
  \centering
     \includegraphics[width=0.5\linewidth]{images/freeze_deadlock_ok.png}
  \caption{Since the boxes in this level are frozen on top of the goals, no freeze deadlock is created.}
  \label{fig:fok}
\end{figure}

The algorithm for detecting freeze deadlocks is rather intuitive, since we only need to check whether a box can be pushed. Since a freeze deadlock is created after pushing a block, it is only neccessary to check if the pushed box is frozen, and possibly the boxes around the pushed box. The following algorithm will identify whether or not pushing a box into a position will create a freeze deadlock. \cite{Wiki}

\begin{algorithm}
  \caption{Identifying freeze deadlocks}
\begin{algorithmic}[1]
  \State $frozen \gets \textnormal{empty set}$
  \State $visited \gets \textnormal{empty set}$
  \Function {Frozen}{position}
    \State $\textnormal{add position to visited}$
    \If { wall on left or right }
      \State $h\gets true$
    \ElsIf { simple deadlock on on left and right }
      \State $h\gets true$
    \ElsIf { box on left or right }
      \State $h\gets \textnormal{Frozen(left) or Frozen(right)}$
    \EndIf

    \If { wall on up or down }
      \State $v\gets true$
    \ElsIf { simple deadlock on on up and down }
      \State $v\gets true$
    \ElsIf { box on up or down }
      \State $v\gets \textnormal{Frozen(up) or Frozen(down)}$
    \EndIf
    \State \Return $\textnormal{h and v and all frozen on goals}$
  \EndFunction
  \end{algorithmic}
\end{algorithm}

%------------------------------------------------------------------------- 
\SubSection{Motivation}

The problem of solving a sokoban puzzle is often compared to the real world problem of programming an autonomous robot to work in a warehouse. Such a robot would be required to navigate the warehouse, as well as perform its designated tasks, using only the information in its immediate vicinity. If such a robot’s task were to move crates into a storage location, then the two problems would be synonymous. In this case, a similar solution could be used to solve both problems. If Q-Learning proves to be a valid solution to the sokoban problem, it’s application to real world problems could prove to be limitless.

If, however, there was more than a single agent acting within the environment, the basic Q-Learning approach will fail. For example, imagine a situation where multiple autonomous robots are each moving boxes within a warehouse from location to location. If a basic Q-Learning approach was implemented, these robots would begin to interfere with one another. To fix this, the robots would need some way to communicate learned information to and from one another [7].

%------------------------------------------------------------------------- 
\SubSection{Notation}

%------------------------------------------------------------------------- 
\Section{Related Work}

Sokoban has been proven to be a PSPACE-Complete problem [1]. Other approaches to this problem include BFS, DFS, and solvers employing heuristic functions such as A* search.

%------------------------------------------------------------------------- 
\Section{Q-Learning}

Q-Learning is a form of model-free reinforcement learning \cite{Watkins1992}. This techniques can be used to find the optimal action policy for any given markov decision process. A markov dicision process is a mathematical framework used to model decision making problems where the decision are under the agent's control. In Q-Learning, an agent experiences the following sequence of actions until the optimal action policy is identified. At any given state $s_t$ the agent can perform an action $a$. The result of taking action $a_t$ is the new state $s_{t+1}$. The agent recieved a immediate reward of $R_a(s_t, s_{t+1})$ for taking action $a$. Using this reward, the agent adjusts its $Q(s, a)$ values using the following equation. \cite{Watkins1992}

\begin{equation}
\label{eq:q}
\begin{split}
Q(s_t, a_t) \gets &Q(s_t, a_t) \\
                  &+ \alpha \cdot [R_a(s_t, s_{t+1}) \\
                  &+ \gamma \cdot max_aQ(s_{t+1}, a) - Q(s_t, a_t)]
\end{split}
\end{equation}

where 

\begin{equation}
max_aQ(s_{t+1}, a)
\end{equation}

is the maximum $Q(s, a)$ value that the agent can achieve from state $s_{t+1}$. This is determined by maximizing the $Q(s_{t+1}, a)$ value for every possible action that can be performed on state $s_{t+1}$. In the case of Sokoban, this means maximizing on the actions: up, down, left, and right. This results in the agents ability to select actions that will produce the greatest reward. In a sense, the agent is maximizing the reward that they recieve. 

In equation \ref{eq:q}, the learning rate is described using $\alpha \in (0, 1)$, and the discount factor is described using $\gamma \in (0, 1)$. The lewarning rate represents the rate at which the agent updates the $Q(s, a)$ values. When the learning rate is 0, the values will never change, meaning the agent will not learn anything. When the learning rate is 1, the agent will consider only the most recent information \cite{Littman94markovgames}. The discount factor controls the effect of future rewards in the decision making of the agent \cite{Littman94markovgames}. A discount factor of 0 will cause the agent to place emphasis on current rewards, and a discount factor of 1 will cause the agent to pace emphasis on future rewards \cite{Littman94markovgames}. Our implementation utilzed a learning rate of 0.1 and a discount factor of 0.9. It is worth mentioning that the optimal learning rate and discount factor were not used in our implementation.

Q-Learning allows a single agent, aware of its current state, to develop a strategy through exploration. This is accomplished through consideration of past states and future reward for performing all possible actions on the current state. An agent is placed into its environment at an initial state $s$ and begins by taking some arbitrary action $a$. Once this action is taken, the Q-value for the given state $s$, after taking that action $a$, is updated. The agent is said to have learned from this action $a$, and will use this learned reward in the future if it encounters this state $s$ again. 

%------------------------------------------------------------------------- 
\SubSection{Rewards}

Rewards are one of the major building blocks of Q-Learning.  If you recall, a reward is given to an agent once an action $a$ is perfomed on a state $s_t$.  Many Q-Learning algorithms implement a reward function which, depending on the outcome state $s_{t+1}$, will result in a numberical reward.  These rewards will fall into either one of two categories, negative or positive.  A positive reward should be given when a desirable outcome is acheived after action $a_t$ is performed at state $s_t$, resulting in an outcome state $s_{t+1}$.  The nature of the rewards implementation in the Q-Learning algorithm will cause the agent to perfer taking action $a$ at state $s_t$ again.  A negative reward should be given when a undesirable outcome is acheived after action $a$ is performed at state $s_t$, resulting in an outcome state $s_{t+1}$.  Converse to the effect that the positive reward will have, the negative reward will cause the agent to avoid taking action $a$ at state $s_t$ again.  The formula for this reward is generally written as a difference function between the intial state $s_t$ and outcome state $s_{t+1}$ as seen in equation \ref{eq:reward}.

\begin{equation}
reward \gets R_a(s_t, s_{t+1})
\label{eq:reward}
\end{equation}

A list of the most recent rewardsss given to our Sokoban agent for transitioning from state $s_t$ to outcome state $s_{t+1}$ can be seen in table \ref{table:rewards}.  We found that positive rewards should be given very rarely, and only in extremely desirable circumstances.  For instance, we gave the highest (positive) reward when a box was pushed onto a goal.  In the inital design of our solution, we gave the agent a reward of 0 for moving, because we saw no immediate negative or positive effects from this.  However, in our final implementation, we gave a negative reward to an agent for moving because this encouraged exploration, discouraged 'wondering', and prevented feedback loops.  A feedback loop occurs when the agent learns a path which does not lead to solving the puzzle, simply because the agent receives the highest reward for this.  For example, the agent would push the same box onto and off of the same goal repeatidly because this lead to the highest reward.  In the development of a successful Q-Learning algorithm for the Sokoban puzzle, we found that the adjustment of our rewards had a significant impact on the success rate of the solver.

\begin{table}[htbp]
  \centering
  \begin{tabular}{l c} \hline\hline
    Transition & Reward \\ \hline
    Box pushed onto goal & $+1.00$ \\
    Box pushed & $-0.01$ \\
    Agent moved & $-0.15$ \\
    Box pushed against wall & $-0.15$ \\
    Box pushed against box & $-0.15$ \\
    Box pushed from one goal to another & $-0.15$ \\
    Box pushed off goal & $-0.20$ \\ 
    Agent moved into wall & $-1.00$ \\
    Agent moved into immovable box & $-1.00$ \\
    Agent created simple deadlock & $-1.00$ \\
    Agent created freeze deadlock & $-1.00$ \\ \hline\hline
  \end{tabular}
  \caption{Immediate rewards for transitioning a Sokoban level.}
  \label{table:rewards}
\end{table}

%------------------------------------------------------------------------- 
\SubSection{Algorithm}

Algorithm \ref{alg:q_learning} represents the entry point for the Sokoban solver.  An initial state $s_0$ and desired episodes $e$ will be used to run an iterative learning process on the state.  An episode represents one iteration of learning from intial state to terminal state.  A larger number of episodes run during Q-Learning will result in a more optimal solution.  Also, though unimplemented in our solver, when the Q values converge, meaning they stop changing between episodes, hte optimal solution has been found.

\begin{algorithm}
  \caption{Solver for Sokoban using Q-learning}
  \begin{algorithmic}[2]
    \Function {QLearning}{state, episodes}
      \State $Q\gets \textnormal{empty set}$
      \For {episode = episodes}
        \State $Q\gets \textnormal{RunEpisode(Q)}$
      \EndFor
      \Return Q
    \EndFunction
  \end{algorithmic}
\label{alg:q_learning}
\end{algorithm}

Algorithm \ref{alg:run_episode} [3][6] is where the majority of the Q-Learning is completed, and is used to determine the optimal strategy for solving a given Sokoban puzzle.  We run an episode of Q-Learning on an initial state with persistant Q values.  While state $s_t$ is not terminal, we pick an action which will maximize the Q value that can be reach, since this is by definition the highest rewarded action that the agent can take at $s_t$.  Once we take action $a$, we calculate the outcome state $s_{t+1}$ and update the Q value for $s_t$ taking action $a$.  This update of the Q value is effectivly causing the agent to learn based on the action that is taken.

\begin{algorithm}
  \caption{Runs a single episode of q learning}
  \begin{algorithmic}[3]
    \Function {RunEpisode}{state, Q}
      \State $state\gets \textnormal{initial state}$
      \While {not Terminal?(state)}
        \State $action\gets \textnormal{MaximizeAction(state, Q)}$
        \State $resultState\gets \textnormal{TakeAction(state, action)}$
        \State $\textnormal{update Q value}$
      \EndWhile
      \State \Return Q
    \EndFunction
  \end{algorithmic}
  \label{alg:run_episode}
\end{algorithm}

Algorithm \ref{alg:maximize_q} is used to calculate the maximum Q value that can be reached in a single action $a$ from state $s_t$.  A simple iteration over the actions possible at that state will yeild the maximum Q value that is reachable.  Similarly, Algorithm \ref{alg:maximize_action} is used to find the action which will yeild the maximum Q value.  If many action will result in an equal Q value, we randomly pick one.  In the sokoban puzzle, these two algorithms are the maim logic for the agent to use its learned knowlege to perform actions reulting in the highest possible reward.  Since the sokoban agent can take only one of four possible actions - up, down, left, and right - they must look at each of these and make an informed decision to which will produce the highest reward.  If many of these actions will result in the same value, the agent will randomly select one of them.

\begin{algorithm}
  \caption{Returns the maximum q value reachable from a state in a single action}
  \begin{algorithmic}[4]
    \Function {MaximizeQ}{state, Q}
      \State $max\gets -\infty$
      \For {action = up, right, down, left}
        \If {$Q(state, action) > max$}
          \State $max\gets Q(state, action)$
        \EndIf
      \EndFor
      \State \Return $max$
    \EndFunction
  \end{algorithmic}
  \label{alg:maximize_q}
\end{algorithm}

\begin{algorithm}
  \caption{Returns the action which will acheive the maximum q value reachable from a state}
  \begin{algorithmic}[5]
    \Function {MaximizeAction}{state, Q}
      \State $max\gets -\infty$
      \State $actions\gets \textnormal{empty set}$
      \For {action = up, right, down, left}
        \If {$Q(state, action) > max$}
          \State $max\gets \textnormal{Q(state, action)}$
          \State $actions\gets \textnormal{set with action}$
        \Else
          \State $actions\gets actions + action$
        \EndIf
      \EndFor
      \State \Return random action from $actions$
    \EndFunction
  \end{algorithmic}
  \label{alg:maximize_action}
\end{algorithm}

Algorithm \ref{alg:take_action} is used to perform action $a$ on state $s_t$ which will produce an outcome state $s_{t+1}$.  Because this is highly dependent on the implementation your Sokoban puzzle, this has been left, for the most part, unimplemented.  In our final implementation of the Sokoban solver, we encorperated our reward function into this algorithm.  When action $a$ is performed on state $s_t$ it produces an oucome state $s_{t+1}$ as well as returning an appropriate reward for the transition.

\begin{algorithm}
  \caption{Returns the resulting state after an action is taken on an intial state}
  \begin{algorithmic}[6]
    \Function {TakeAction}{state, action}
      \State \Return state after action is taken
    \EndFunction
  \end{algorithmic}
  \label{alg:take_action}
\end{algorithm}

%------------------------------------------------------------------------- 
\Section{Experimental Analysis}

In order to evaluate the performance of our solver, we selected three levels with various difficulty. The levels were classified into three groups, easy, medium, and hard. These difficulties are based on the minimum number of moves required to solve the puzzle, and the number of boxes in the puzzle. In order to understand the exact capabilities of the solver, the following parameters were measured after each episode: the number of moves the agent takes, the average reward the agent received, whether or not the agent was able to solve the puzzle, and the time it took for the agent to complete the episode. 

\begin{table}[htbp]
  \centering
  \begin{tabular}{l c c c} \hline\hline
    Difficulty & Boxes & Moves & Episodes \\ \hline
    Easy & 1 & 11 & 70 \\
    Medium & 2 & 9 & 140 \\
    Hard & 2 & 30 & 400 \\ \hline\hline
  \end{tabular}
  \caption{Results from solving 3 different puzzles.}
  \label{table:results}
\end{table}

\begin{figure}[h] 
  \centering
     \includegraphics[width=\linewidth]{images/easy_graph.png}
  \caption{Results for solving an easy level.}
  \label{fig:e}
\end{figure}

The results from figure \ref{fig:e} demonstrate the solver's ability to identify the optimal action policy after only 100 episodes. Although the number of moves the solver used to solve the puzzle stabilized at 11 after only 70 episodes, the average reward for each episode did not stabilize until episode 100. Instead, the average reward continued to increase until its maximum at episode 100. This demonstrates the ability of the solver to refine the action policy through continuous exploration. Although the agent identified an action policy that would solve the puzzle, they continued to search for an even better action policy. However, since no better action policy was found, the agent continued to use the action policy that they knew would produce the greatest reward.

\begin{figure}[h] 
  \centering
     \includegraphics[width=\linewidth]{images/medium_graph.png}
  \caption{Results for solving an medium level.}
  \label{fig:m}
\end{figure}

The level that produced the results in figure \ref{fig:m} was a bit more challenging than the level that produced the results to figure \ref{fig:e}. Though the number of moves required to solve the level was only 9, compared to the 11 moves required to solve the other level, the number of boxes increased to two. This increases the complexity of the level greatly. After about 140 episode, the solver was able to identify the optimal action policy.  

\begin{figure}[h] 
  \centering
     \includegraphics[width=\linewidth]{images/hard_graph.png}
  \caption{Results for solving an hard level.}
  \label{fig:h}
\end{figure}

The level that produced the results in figure \ref{fig:h} takes a minimum of 30 moves to solve and contained two boxes, making it the most complicated level that was tested. The solver was able to identify the optimal action policy after about 400 episodes. The results from this data provided the most insight into the solvers performance. The number of moves that the solver used began small, increasing until about episode 80, then decreasing until about episode 400. If the solvers goal is to minimize the number of moves, why was the number of moves increasing? Though the number of moves was increasing, the agent was not solving the level. instead, the agent was creating deadlocks and the episode was ending early. This allowed the agent to identify the actions that created deadlocks and increase the number of moves they were able to take. When all deadlocks were identified, after approximately 80 episodes, the agent could then refine the action policy and reduce the overall moves they needed to take.

\SubSection{Rewards}

%------------------------------------------------------------------------ 
\Section{Conclusions}

%------------------------------------------------------------------------- 
\bibliographystyle{latex8}
\bibliography{latex8}

\begin{thebibliography}{9}
\bibitem{wiki}
Sokoban wiki.
\url{http://www.sokobano.de/wiki}.


\bibitem{culberson} 
J. Culberson. 
\textit{Sokoban is PSPACE-complete}.
1997.

\bibitem{jarusek}
P. Jarušek and R. Pelánek.
Difficulty rating of sokoban puzzle
in Proc. of the Fifth Starting AI Researchers' Symposium (STAIRS 2010).
Dec, 2010. pp.~140-150

\bibitem{russell}
S. J. Russell and P. Norvig.
\textit{Artificial intelligence: a modern approach}.
Upper Saddle River: Prentice-Hall, 2010.

\bibitem{littman} 
M. L. Littman.
Markov games as a framework for multi-agent
reinforcement learning in Proceedings of the eleventh international
conference on machine learning. Vol 157. pp 157-163. 1994

\bibitem{guo} 
M. Guo, Y. Liu and J. Malec
\textit{A new Q-learning algorithm based on the metropolis criterion}.
IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol.~34, no. 5, pp.~2140-2143, Oct. 2004.

\bibitem{ming} 
T. Ming.
\textit{Multi-agent reinforcement learning: Independent vs.~cooperative agents.}.
Proceedings of the tenth international conference on machine learning. 1993.

\end{thebibliography}

\end{document}


